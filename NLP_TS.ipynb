{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1NMqTdzIFoVBvRFkZ5wFNJiWxyiI5B4bl","authorship_tag":"ABX9TyNWgc2clwnOnHbwHEb3Dcio"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!unzip \"/content/drive/MyDrive/fine_tuned_led_large_16384.zip\" -d \"/content/drive/MyDrive/\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oPyrlgUNRDO5","executionInfo":{"status":"ok","timestamp":1708260641165,"user_tz":-330,"elapsed":15648,"user":{"displayName":"Pippalla Sai Prasanna Kumar","userId":"09495847165964359047"}},"outputId":"d6286794-3801-49a6-8169-047df5debd40"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  /content/drive/MyDrive/fine_tuned_led_large_16384.zip\n","  inflating: /content/drive/MyDrive/fine_tuned_led_large_16384/config.json  \n","  inflating: /content/drive/MyDrive/fine_tuned_led_large_16384/generation_config.json  \n","  inflating: /content/drive/MyDrive/fine_tuned_led_large_16384/model.safetensors  \n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dbantx3BOvHo","executionInfo":{"status":"ok","timestamp":1708260649683,"user_tz":-330,"elapsed":5920,"user":{"displayName":"Pippalla Sai Prasanna Kumar","userId":"09495847165964359047"}},"outputId":"47fea42b-31f5-4205-d357-0e271286c897"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.2)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.2)\n","Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu121)\n","Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.27.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.3)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n"]}],"source":["!pip install transformers[torch]"]},{"cell_type":"code","source":["##############   Preprocessing    #####################\n","# i have run this code in jupiter .. don't run here\n","#output file is saved in drive\n","#2)\n","\n","import os\n","import csv\n","import re\n","from nltk.tokenize import word_tokenize\n","\n","def preprocess_text(text):\n","    text = text.lower()\n","    text = re.sub(r\"[^a-zA-Z0-9\\s$€£¥\\n\\r]\", \"\", text)\n","    text = re.sub(r\"\\s+\", \" \", text)\n","    return text\n","\n","def extract_tokens(text, max_tokens):\n","    tokens = word_tokenize(text)\n","    return tokens[:max_tokens]\n","\n","def divide_text_into_parts(tokens, num_parts):\n","    part_size = len(tokens) // num_parts\n","    divided_parts = [tokens[i * part_size: (i + 1) * part_size] for i in range(num_parts)]\n","    return divided_parts\n","\n","def find_best_summary(report_tokens, golden_summaries):\n","    best_summary = None\n","    best_intersection = 0\n","    selected_summary_number = None\n","\n","    for summary_index, summary in enumerate(golden_summaries, start=1):\n","        intersection = len(set(report_tokens) & set(summary))\n","        if intersection > best_intersection:\n","            best_intersection = intersection\n","            best_summary = summary\n","            selected_summary_number = summary_index\n","\n","    return best_summary, selected_summary_number\n","\n","dataset_folder = \"fns_2020/validation\"\n","output_csv_file = \"val_preprocessed_data.csv\"\n","\n","count = 0\n","error_files = []\n","\n","with open(output_csv_file, mode='w', newline='', encoding='utf-8') as csvfile:\n","    csv_writer = csv.writer(csvfile)\n","    csv_writer.writerow(['input_report', 'selected_summary_number', 'generated_summary', 'report_file_path'])\n","\n","    for report_file in os.listdir(os.path.join(dataset_folder, 'annual_reports')):\n","\n","        report_path = os.path.join(dataset_folder, 'annual_reports', report_file)\n","        try:\n","            with open(report_path, 'r', encoding='utf-8') as f:\n","                report_text = f.read()\n","                report_text = preprocess_text(report_text)\n","                report_tokens = extract_tokens(report_text, 8192)\n","\n","            golden_summary_folder = os.path.join(dataset_folder, 'gold_summaries')\n","            golden_summaries = []\n","            for summary_file in os.listdir(golden_summary_folder):\n","                if summary_file.startswith(os.path.splitext(report_file)[0]):\n","                    summary_path = os.path.join(golden_summary_folder, summary_file)\n","                    with open(summary_path, 'r', encoding='utf-8') as f:\n","                        summary_text = f.read()\n","                        summary_text = preprocess_text(summary_text)\n","                        summary_tokens = extract_tokens(summary_text, 1024)\n","                        golden_summaries.append(summary_tokens)\n","\n","            best_summary_tokens, selected_summary_number = find_best_summary(report_tokens, golden_summaries)\n","            divided_report_parts = divide_text_into_parts(report_tokens, 8)\n","\n","            for i, report_part in enumerate(divided_report_parts, start=1):\n","                csv_writer.writerow([' '.join(report_part), selected_summary_number, ' '.join(best_summary_tokens), report_file])\n","                count += 1\n","                print(count)\n","\n","        except Exception as e:\n","            print(f\"Error processing file {report_path}: {str(e)}\")\n","            error_files.append(report_path)"],"metadata":{"id":"887HwJ9Opolk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XZ93VijQpojV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from transformers import LEDForConditionalGeneration, LEDTokenizer\n"],"metadata":{"id":"OnNtFAP_O6-d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","#from datasets import Dataset, load_dataset\n","from torch.utils.data import DataLoader\n","from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n","\n","# Load input data and generated summaries from CSV\n","data_df = pd.read_csv('/content/drive/MyDrive/preprocessed_data.csv')  # Update with the path to your CSV file\n","\n","#-----------------------------------------------------------------------------------------------------------------------\n"],"metadata":{"id":"NUOZtecCO9ju"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Assuming your CSV file has columns named 'input_data' and 'generated_summary'\n","input_data = data_df['input_report'].tolist()\n","generated_summary = data_df['generated_summary'].tolist()\n","print(f\"Input data {input_data}\")\n","# Tokenization\n","tokenizer = LEDTokenizer.from_pretrained(\"allenai/led-base-16384\")\n","\n","\n","# Define the maximum input and output lengths\n","max_input_length = 1024\n","max_output_length = 128\n","# Tokenize input and output data\n","#tokenized_data = tokenizer(input_data, generated_summary, max_length=max_input_length, padding=\"max_length\", truncation=True)\n","# Tokenize input and output data\n","tokenized_data = tokenizer(input_data, max_length=max_input_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n","\n","# Update tokenized_data to include labels\n","tokenized_data['labels'] = tokenizer(generated_summary, max_length=max_output_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")['input_ids']\n","\n","# Ensure that the tokenized data contains 'input_ids' and 'labels'\n","input_ids = tokenized_data['input_ids']\n","labels = tokenized_data['labels']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ox4s6qdVPE5h","executionInfo":{"status":"ok","timestamp":1708261052067,"user_tz":-330,"elapsed":364581,"user":{"displayName":"Pippalla Sai Prasanna Kumar","userId":"09495847165964359047"}},"outputId":"a26c72c3-cd63-4757-f3ec-bfdd8fae9d1c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["IOPub data rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_data_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# Load input data and generated summaries from CSV\n","val_data_df = pd.read_csv('/content/drive/MyDrive/val_preprocessed_data.csv')  # Update with the path to your CSV file"],"metadata":{"id":"zWXSIBfNPJgE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Assuming your CSV file has columns named 'input_data' and 'generated_summary'\n","val_input_data = val_data_df['input_report'].tolist()\n","val_generated_summary = val_data_df['generated_summary'].tolist()\n","print(f\"Input data {val_input_data}\")\n","# # Tokenization\n","# tokenizer = LEDTokenizer.from_pretrained(\"allenai/led-base-16384\")\n","\n","# Define the maximum input and output lengths\n","max_input_length = 1024\n","max_output_length = 128\n","# Tokenize input and output data\n","#tokenized_data = tokenizer(input_data, generated_summary, max_length=max_input_length, padding=\"max_length\", truncation=True)\n","# Tokenize input and output data\n","val_tokenized_data = tokenizer(val_input_data, max_length=max_input_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n","\n","# Update tokenized_data to include labels\n","val_tokenized_data['labels'] = tokenizer(val_generated_summary, max_length=max_output_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")['input_ids']\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dkU4QtBVPRFn","executionInfo":{"status":"ok","timestamp":1708261091847,"user_tz":-330,"elapsed":39148,"user":{"displayName":"Pippalla Sai Prasanna Kumar","userId":"09495847165964359047"}},"outputId":"9dc87296-d605-42eb-cd72-b2e4fc19dcea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["IOPub data rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_data_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n"]}]},{"cell_type":"code","source":["# don't run this command if u have already installed it\n","!pip install datasets\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":932},"id":"Zux7OW0qPXdb","executionInfo":{"status":"ok","timestamp":1708260557667,"user_tz":-330,"elapsed":11693,"user":{"displayName":"Pippalla Sai Prasanna Kumar","userId":"09495847165964359047"}},"outputId":"c4f7ab95-1920-4619-d07c-e7fc1186cf0a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-2.17.0-py3-none-any.whl (536 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.6/536.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n","Collecting pyarrow>=12.0.0 (from datasets)\n","  Downloading pyarrow-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n","Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: pyarrow, dill, multiprocess, datasets\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 10.0.1\n","    Uninstalling pyarrow-10.0.1:\n","      Successfully uninstalled pyarrow-10.0.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 15.0.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-2.17.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-15.0.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pyarrow"]}}},"metadata":{}}]},{"cell_type":"code","source":["from datasets import Dataset\n","\n","# Prepare the dataset\n","dataset = Dataset.from_dict(tokenized_data)\n","val_dataset = Dataset.from_dict(val_tokenized_data)"],"metadata":{"id":"aeBxr3CqPZ4u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Model Configuration\n","# model = LEDForConditionalGeneration.from_pretrained(\"allenai/led-base-16384\")\n","\n","model_name_or_path = \"/content/drive/MyDrive/fine_tuned_led_large_16384\"\n","\n","# Load tokenizer\n","tokenizer = LEDTokenizer.from_pretrained(\"allenai/led-base-16384\")\n","\n","\n","# Load model\n","model = LEDForConditionalGeneration.from_pretrained(model_name_or_path)\n","\n","\n","# Fine-Tuning\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir='./results',\n","    num_train_epochs=3,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    save_steps=10_000,\n","    save_total_limit=2,\n","    predict_with_generate=True,\n","    evaluation_strategy=\"steps\",\n","    eval_steps=500,  # Evaluate every 500 steps\n",")\n","\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=dataset,\n","    eval_dataset=val_dataset,  # Specify the evaluation dataset\n",")"],"metadata":{"id":"PJvvIgIbPb3f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#training\n","trainer.train()\n","\n","# Save the Model\n","model.save_pretrained(\"fine_tuned_led_large_16384\")\n"],"metadata":{"id":"tkopBmFNRd9G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd=trainer.predict(val_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":72},"id":"TPW1vvTqTDQi","executionInfo":{"status":"ok","timestamp":1708261678681,"user_tz":-330,"elapsed":517392,"user":{"displayName":"Pippalla Sai Prasanna Kumar","userId":"09495847165964359047"}},"outputId":"f281ad02-471c-4b3a-d199-9fc945632918"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}}]},{"cell_type":"code","source":["print(pd)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5wI9MYu-Vu19","executionInfo":{"status":"ok","timestamp":1708261835065,"user_tz":-330,"elapsed":8,"user":{"displayName":"Pippalla Sai Prasanna Kumar","userId":"09495847165964359047"}},"outputId":"003f5811-6ce2-406b-9d1c-a866bd7dc302"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["PredictionOutput(predictions=array([[    2,     0,  3387, ...,   266,    52,   548],\n","       [    2,     0,  4124, ...,  6149,    29,  3744],\n","       [    2,     0, 15922, ...,   266,     8,  2349],\n","       ...,\n","       [    2,     0,  6031, ...,   834,  4585,  1551],\n","       [    2,     0,  1570, ...,    52,    33,    10],\n","       [    2,     0,  1366, ...,   806,    24,    70]]), label_ids=array([[    0,   698, 14382, ...,  7482,  1633,     2],\n","       [    0,   698, 14382, ...,  7482,  1633,     2],\n","       [    0,   698, 14382, ...,  7482,  1633,     2],\n","       ...,\n","       [    0,  4124, 13550, ...,  3156,    11,     2],\n","       [    0,  4124, 13550, ...,  3156,    11,     2],\n","       [    0,  4124, 13550, ...,  3156,    11,     2]]), metrics={'test_loss': 2.970895528793335, 'test_runtime': 516.8134, 'test_samples_per_second': 5.619, 'test_steps_per_second': 0.702})\n"]}]},{"cell_type":"code","source":["######"],"metadata":{"id":"THW-d3BRTLWs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#working fine to add one cell in test.csv file\n","\n","# Load \"val_preprocessed_data.csv\" and get the \"generated_summary\" and \"report_file_path\" columns\n","with open(\"/content/drive/MyDrive/val_preprocessed_data.csv\", \"r\", encoding='utf-8') as csvfile:\n","    csv_reader = csv.DictReader(csvfile)\n","    rows = list(csv_reader)\n","    generated_summary = rows[0][\"generated_summary\"]  # Take the first generated summary\n","    report_file_path = rows[0][\"report_file_path\"]  # Take the first report file path\n","\n","# Example predictions and label_ids\n","# pd = trainer.predict(val_dataset)\n","predictions = pd.predictions\n","label_ids = pd.label_ids\n","\n","# Decode the first 8 arrays in label_ids\n","decoded_text = \"\"\n","for labels in label_ids[:8]:\n","    decoded_text += tokenizer.decode(labels)\n","\n","# Write data to the CSV file\n","with open(\"test.csv\", \"w\", newline=\"\", encoding='utf-8') as csvfile:\n","    fieldnames = [\"filename\", \"generated_text\", \"generated_summary\"]\n","    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n","    writer.writeheader()\n","\n","    # Write the first filename, decoded text, and generated summary to the CSV file\n","    writer.writerow({\"filename\": report_file_path, \"generated_text\": decoded_text, \"generated_summary\": generated_summary})\n"],"metadata":{"id":"-2GYuTiJchxX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"iCFQpYYYdjJj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#this code is working fine for test.csv\n","import csv\n","\n","# Load \"val_preprocessed_data.csv\" and get the \"generated_summary\" and \"report_file_path\" columns\n","with open(\"/content/drive/MyDrive/val_preprocessed_data.csv\", \"r\", encoding='utf-8') as csvfile:\n","    csv_reader = csv.DictReader(csvfile)\n","    rows = list(csv_reader)\n","\n","# Example predictions and label_ids\n","# pd = trainer.predict(val_dataset)\n","predictions = pd.predictions\n","label_ids = pd.label_ids\n","\n","# Write data to the CSV file\n","with open(\"test.csv\", \"w\", newline=\"\", encoding='utf-8') as csvfile:\n","    fieldnames = [\"filename\", \"generated_text\", \"generated_summary\"]\n","    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n","    writer.writeheader()\n","\n","    idx = 0\n","    while idx < len(rows):\n","        # Take one value from \"report_file_path\" and \"generated_summary\" from the current set of rows\n","        report_file_path = rows[idx][\"report_file_path\"]\n","        generated_summary = rows[idx][\"generated_summary\"]\n","\n","        # Decode the next 8 arrays in label_ids\n","        decoded_text = \"\"\n","        for labels in label_ids[idx:idx+8]:\n","            decoded_text += tokenizer.decode(labels)\n","\n","        # Write the filename, decoded text, and generated summary to the CSV file\n","        writer.writerow({\"filename\": report_file_path, \"generated_text\": decoded_text, \"generated_summary\": generated_summary})\n","\n","        # Move to the next set of 8 rows in val_preprocessed_data.csv\n","        idx += 8\n"],"metadata":{"id":"tPBX8moYchcT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EP5ryLdagNah"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#rouge scores calculation"],"metadata":{"id":"XWwWqEYSjjxy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import csv\n","!pip install rouge-score\n"],"metadata":{"id":"El4klF_sji4d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from rouge_score import rouge_scorer\n","\n","# Function to calculate all ROUGE scores\n","def calculate_rouge_scores(generated_text, generated_summary):\n","    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","    scores = scorer.score(generated_text, generated_summary)\n","    rouge_scores = f\"ROUGE-1: {scores['rouge1'].fmeasure:.4f}, ROUGE-2: {scores['rouge2'].fmeasure:.4f}, ROUGE-L: {scores['rougeL'].fmeasure:.4f}\"\n","    return rouge_scores\n"],"metadata":{"id":"8E4jgb-xn5kZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#working fine\n","# Read data from the existing CSV file\n","input_csv_file = \"/content/drive/MyDrive/test.csv\"\n","output_csv_file = \"updated_file.csv\"\n","\n","with open(input_csv_file, \"r\", encoding=\"utf-8\") as csv_file:\n","    csv_reader = csv.DictReader(csv_file)\n","    rows = list(csv_reader)\n","\n","# Update data and write to the new CSV file\n","with open(output_csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n","    fieldnames = [\"filename\", \"generated_text\", \"generated_summary\", \"rouge\"]\n","    csv_writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n","    csv_writer.writeheader()\n","\n","    for row in rows:\n","        # Calculate all ROUGE scores for each row\n","        rouge_scores = calculate_rouge_scores(row[\"generated_text\"], row[\"generated_summary\"])\n","\n","        # Update the row with all ROUGE scores\n","        row[\"rouge\"] = rouge_scores\n","\n","        # Write the updated row to the new CSV file\n","        csv_writer.writerow(row)\n","\n","print(\"CSV file updated successfully with all ROUGE scores.\")"],"metadata":{"id":"cOwhELFfn--3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LsBqTKEsYtZC"},"execution_count":null,"outputs":[]}]}